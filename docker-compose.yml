services:
  postgres:
    container_name: nakama-postgres
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: nakama
      POSTGRES_PASSWORD: localdb
      POSTGRES_USER: postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
    expose:
      - "8080"
      - "5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  nakama:
    container_name: nakama
    image: heroiclabs/nakama:3.25.0
    depends_on:
      postgres:
        condition: service_healthy
    entrypoint:
      - "/bin/sh"
      - "-ecx"
      - >
        /nakama/nakama migrate up --database.address postgres:localdb@postgres:5432/nakama &&
        exec /nakama/nakama --name nakama1
        --database.address postgres:localdb@postgres:5432/nakama
        --runtime.path /local/backend/modules
        --logger.level INFO
    volumes:
      - ./:/local
    ports:
      - "7349:7349"   # Nakama gRPC
      - "7350:7350"   # Nakama HTTP API
      - "8080:7351"   # Nakama Console (admin UI; container listens on 7351)
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/nakama/nakama", "healthcheck"]
      interval: 10s
      timeout: 5s
      retries: 5

  generation-service:
    build: ./backend/generation-service
    container_name: card-generation-service
    environment:
      ASSETS_DIR: /app/assets
      CATALOG_PATH: /app/data/card-templates.json
      # Set OPENAI_API_KEY (e.g. in a .env file) to use AI generation instead of placeholders
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      # Debug: raw AI responses written here (remove later)
      DEBUG_AI_OUTPUT_PATH: /debug_output/ai_response_debug.txt
    volumes:
      - ./backend/assets:/app/assets
      - ./backend/data:/app/data
      - ./backend/generation-service/debug_output:/debug_output
    ports:
      - "8000:8000"
    restart: unless-stopped

volumes:
  postgres_data:
